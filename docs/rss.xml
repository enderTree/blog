<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>ender'Blog</title><link>https://enderTree.github.io/blog</link><description>闭关ING</description><copyright>ender'Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://enderTree.github.io/blog</link></image><lastBuildDate>Thu, 26 Jun 2025 16:22:19 +0000</lastBuildDate><managingEditor>ender'Blog</managingEditor><ttl>60</ttl><webMaster>ender'Blog</webMaster><item><title>MaxFrame实现minhash Dedup</title><link>https://enderTree.github.io/blog/post/MaxFrame-shi-xian-minhash%20Dedup.html</link><description>```python
import re
from itertools import tee
from typing import Iterable, Set, List

import numpy as np
import maxframe.dataframe as md
from maxframe import options
from maxframe.session import new_session
from maxframe.udf import with_python_requirements
from maxframe.learn.contrib.graph import connected_components
from odps import ODPS

import logging

logging.basicConfig(level=logging.INFO)

SEED = 42
NON_ALPHA = re.compile('[^A-Za-z_0-9]')
RNG = np.random.RandomState(SEED)
MAX_HASH = np.uint64((1 &lt;&lt; 32) - 1)
MERSENNE_PRIME = np.uint64((1 &lt;&lt; 61) - 1)


# 通过相似度阈值和哈希函数个数，生成条带数量和条带行数参数
def optimal_param(
    threshold: float,
    num_perm: int,
    false_positive_weight: float = 0.5,
    false_negative_weight: float = 0.5,
):
    from scipy.integrate import quad as integrate

    def false_positive_probability(threshold: float, b: int, r: int):
        '''Source: `datasketch.lsh`'''

        def proba(s):
            return 1 - (1 - s ** float(r)) ** float(b)

        a, _ = integrate(proba, 0.0, threshold)
        return a

    def false_negative_probability(threshold: float, b: int, r: int):
        '''Source: `datasketch.lsh`'''

        def proba(s):
            return 1 - (1 - (1 - s ** float(r)) ** float(b))

        a, _ = integrate(proba, threshold, 1.0)
        return a

    min_error = float('inf')
    opt = (0, 0)
    for b in range(1, num_perm + 1):
        max_r = int(num_perm / b)
        for r in range(1, max_r + 1):
            fp = false_positive_probability(threshold, b, r)
            fn = false_negative_probability(threshold, b, r)
            error = fp * false_positive_weight + fn * false_negative_weight
            if error &lt; min_error:
                min_error = error
                opt = (b, r)
    return opt


# 依赖 xxhash 完成 hash
@with_python_requirements('xxhash')
def generate_hash_values(row):
    idx = str(row[0])
    content = row[1]

    print(idx, content[:10], flush=True)

    import xxhash

    def ngrams(sequence: List[str], n: int, min_length: int = 5) -&gt; Iterable:
        if len(sequence) &lt; min_length:
            return []

        iterables = tee(sequence, n)
        for i, sub_iterable in enumerate(iterables):
            for _ in range(i):
                next(sub_iterable, None)
        return zip(*iterables)

    def ngram_hashes(content: str, n: int, min_length: int = 5) -&gt; Set[int]:
        import xxhash

        # ========== edit ===========
        tokens: List[str] = NON_ALPHA.split(content.lower())
        ng: set[bytes] = {
            bytes(' '.join(t).lower(), 'utf-8') for t in ngrams(tokens, n, min_length)
        }
        return {xxhash.xxh32_intdigest(n) for n in ng}

    # 获取 hash 生成的排列的参数
    a, b = PERMUTATIONS

    # 获得 ngram hash
    hashes = np.array(
        list(ngram_hashes(content, args_ngram_size, args_min_length)), dtype=np.uint64
    )

    # 如果长度不足，不进行相似度判定
    if len(hashes) == 0:
        return

    # 通过算法为每个 ngram 生成多个 hash 值
    chunk_size = 100
    min_hashes = np.full(args_num_perm, MAX_HASH)
    for i in range(0, len(hashes), chunk_size):
        chunk = hashes[i : i + chunk_size]
        p_hashes = ((np.outer(chunk, a) + b) % MERSENNE_PRIME) &amp; MAX_HASH
        min_hashes = np.minimum(min_hashes, p_hashes.min(axis=0))

    # 生成条带的 hash
    Hs = [bytes(min_hashes[start:end].byteswap().data) for start, end in HASH_RANGES]

    # 返回结果，结果为每个条带返回一行数据
    for band_idx, H in enumerate(Hs):
        yield band_idx, H.hex(), idx

mapper_memory = 4096
reducer_memory = 12288
joiner_memory = 12288
mapper_split_size = 1024
instance_num = 50000

input_table = 'xxx'
output_table = 'xxxx1'

# content_hash
process_column = 'content_hash'  #之后需要改为content
lifecycle = 365
partitions = None  # or None

# 相似度阈值
args_threshold = 0.6  # 0.6
# 文本长度限制
args_min_length = 5  # 5
args_ngram_size = 5  # 5
# hash 函数个数
args_num_perm = 256  # 256

# 关闭查询加速，在这个场景计算时间偏长，不适用mcqa
options.sql.enable_mcqa = False

sql_settings = {
    'odps.namespace.schema': 'true',
    'odps.session.image': 'common',
    'odps.function.timeout': 3600,
    'odps.sql.mapper.memory': mapper_memory,  # 弹内
    'odps.stage.mapper.mem': mapper_memory,  # 弹外
    'odps.sql.reducer.memory': reducer_memory,  # 弹内
    'odps.stage.reducer.mem': reducer_memory,  # 弹外
    'odps.sql.joiner.memory': joiner_memory,  # 弹内
    'odps.stage.joiner.mem': joiner_memory,  # 弹外
    'odps.stage.mapper.split.size': mapper_split_size,
    'odps.sql.cfile2.field.maxsize': 268435456,
    'odps.task.merge.enabled': False,
    'odps.sql.allow.fullscan': 'true',
    'odps.dag2.compound.config': 'fuxi.worker.reuse.policy:NO_REUSE',
    'odps.sql.window.function.newimpl': 'false',
}

# 设置最大列的大小，并发
if instance_num:
    sql_settings.update(
        {
            'odps.sql.split.dop': {input_table: int(instance_num)},
            'odps.stage.reducer.num': 5000,
            'odps.stage.joiner.num': 5000,
            'odps.sql.sys.flag.fuxi_MaxTaskWorkerCount': int(instance_num),
            'odps.sql.max.desired.parallelism': int(instance_num),
            'odps.job.workers.limit': int(instance_num),
        }
    )
options.sql.settings = sql_settings
options.dag.settings.update({'codegen.groupby_first_value_allowed': 'true'})

o = ODPS(
    access_id='',
    secret_access_key='',
    project='xxx',
    # endpoint='http://service.cn-hangzhou.maxcompute.aliyun.com/api',
    endpoint='https://service.cn-hangzhou-vpc.maxcompute.aliyun-inc.com/api',
)

session = new_session(o)
print('session id: ', session.session_id, flush=True)
print('session logview: ', session.get_logview_address(), flush=True)

B, R = optimal_param(args_threshold, args_num_perm)

# 生成 HashRange
HASH_RANGES = [(i * R, (i + 1) * R) for i in range(B)]

# 生成 Hash 函数参数
PERMUTATIONS = np.array(
    [
        (
            RNG.randint(1, MERSENNE_PRIME, dtype=np.uint64),
            RNG.randint(0, MERSENNE_PRIME, dtype=np.uint64),
        )
        for _ in range(args_num_perm)
    ],
    dtype=np.uint64,
).T

df = md.read_odps_table(input_table, partitions=partitions)

# 直接对相同文本去重
df = df.drop_duplicates(subset=[process_column], keep='first')

# # 过滤 text 为空的行, 生成 index 列
df = df.reset_index(drop=False).rename(columns={'index': '_doc_id'})
# 触发计算并缓存结果, 通过 display 预览
df.execute()

# # 投影出所需的数据
docid_docinfo_df = df[['_doc_id', process_column]]

# # 应用 udtf 生成所有的 MinHash 和 LSH
hash_df = docid_docinfo_df.mf.flatmap(
    generate_hash_values,
    dtypes={'band_idx': np.int_, 'hash': np.str_, '_doc_id': np.str_},
    raw=True,
)

hash_df_temp = 'tmp_hash_df'
md.to_odps_table(
    hash_df, hash_df_temp, index=False, overwrite=True, lifecycle=7
).execute()

components_df = md.read_odps_query(
    f'''
    select distinct similar_docs as index, similar_docs as id, _doc_id as component from (
        select cast (_doc_id as bigint) as similar_docs,
                            MIN(cast (_doc_id as bigint)) over(PARTITION BY band_idx, hash) as _doc_id,
                            RANK() over (PARTITION BY band_idx,hash ORDER BY cast (_doc_id as bigint) ASC) as rnk from {hash_df_temp}
    ) where rnk &gt; 1
    ''',
    index_col='index',
)
converged = False

round = 0
# 计算完整联通分量
while not converged:
    round = round + 1
    components_df, converged_scalar = connected_components(
        components_df, 'id', 'component', 6
    )
    session.execute(components_df, converged_scalar)
    converged = converged_scalar.fetch()

components_df = components_df.set_index('id')
result_df = df.merge(components_df, left_on='_doc_id', right_index=True, how='left')
result_df = result_df[result_df['component'].isnull()]
result_df = result_df.drop(['component', '_doc_id'], axis=1)

md.to_odps_table(
    result_df, output_table, lifecycle=lifecycle, overwrite=True, index=False
).execute()

session.destroy()
```。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/MaxFrame-shi-xian-minhash%20Dedup.html</guid><pubDate>Thu, 26 Jun 2025 09:14:22 +0000</pubDate></item><item><title>使用MaxFrame将oss文件入库</title><link>https://enderTree.github.io/blog/post/shi-yong-MaxFrame-jiang-oss-wen-jian-ru-ku.html</link><description>1. oss配置
```python
OSS_ACCESS_ID = ''  # OSS 的 Access ID
OSS_SECRET_ACCESS_KEY = ''  # OSS 的 Secret ID

# 填写 Bucket 信息
OSS_INTERNET_ENDPOINT = (
    'oss-cn-shanghai.aliyuncs.com'  # OSS 的公网访问 endpoint，您也可以使用 sts token
)
OSS_INTERNAL_ENDPOINT = 'oss-cn-shanghai-internal.aliyuncs.com'  # OSS 的公网访问 endpoint，您也可以使用 sts token
OSS_BUCKET_NAME = 'crawler-data-storage'  # OSS Bucket 的名字
OSS_BUCKET_ENDPOINT = f'{OSS_BUCKET_NAME}.{OSS_INTERNET_ENDPOINT}:80'
OSS_BUCKET_ENDPOINT

import oss2
from oss2 import defaults
defaults.connection_pool_size = 50

auth = oss2.Auth(OSS_ACCESS_ID, OSS_SECRET_ACCESS_KEY)
bucket = oss2.Bucket(auth, OSS_INTERNET_ENDPOINT, OSS_BUCKET_NAME)
```
2. 定义oss文件扫描方法
```python
import oss2
from concurrent.futures import ThreadPoolExecutor, as_completed


def get_oss_files(bucket, prefix, ext='.snappy.parquet', max_size=1 * 1024 * 1024 * 1024 * 1024):
    '''
    使用多线程遍历 OSS 路径下的所有目录。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/shi-yong-MaxFrame-jiang-oss-wen-jian-ru-ku.html</guid><pubDate>Thu, 26 Jun 2025 09:11:43 +0000</pubDate></item><item><title>阿里云个人使用的一些姿势</title><link>https://enderTree.github.io/blog/post/a-li-yun-ge-ren-shi-yong-de-yi-xie-zi-shi.html</link><description># 阿里云个人使用的一些姿势

## 一、登录

[登录 dataworks](https://dataworks.console.aliyun.com/workspace/list)，选择对应工作空间进入，显示无空间找管理员开通

&lt;img width='1693' alt='Image' src='https://github.com/user-attachments/assets/b2e470eb-06e2-4c2c-8d12-da6232adc7b2' /&gt;

&lt;img width='1238' alt='Image' src='https://github.com/user-attachments/assets/2408c27a-fcf3-4631-a68a-2f3005c49936' /&gt;

## 二、临时 Sql 运行

临时查询下新建一个目录，在下面新建 odps sql 节点即可

## 三、周期任务

### sql 任务

业务流程 MaxCompute 数据开发目录下新建 ODPS Sql 节点

### PySpark 简单模式

[官方文档](https://help.aliyun.com/zh/maxcompute/user-guide/develop-a-spark-on-maxcompute-application-by-using-pyspark?spm=a2c4g.11186623.0.i2)

1. 资源中新建 python 脚本
   
&lt;img width='443' alt='Image' src='https://github.com/user-attachments/assets/9e325886-5153-4c75-b19b-23fa6476f1b6' /&gt;

 ```python
from pyspark.sql import SparkSession
if __name__ == '__main__':
spark = SparkSession.builder.appName('spark sql').getOrCreate()
spark.sql('SELECT 1').show()
```
2. 数据开发文件夹新建odps spark节点

&lt;img width='735' alt='Image' src='https://github.com/user-attachments/assets/e4a30db5-a9b3-4110-bd83-85c764450f39' /&gt;

发布上线皆可，sparkui会在日志中打印

### PySpark兼容模式

阿里云的PySpark是基于Python 3.7 + Spark2的，而且看上去已经疏于维护了。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/a-li-yun-ge-ren-shi-yong-de-yi-xie-zi-shi.html</guid><pubDate>Wed, 25 Jun 2025 11:06:12 +0000</pubDate></item><item><title>MaxCompute外表使用</title><link>https://enderTree.github.io/blog/post/MaxCompute-wai-biao-shi-yong.html</link><description>### 1. 建表
1. json格式

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS dialogue.dwd_ext_xingye_log_chatml_data(
    `data` STRING
) 
PARTITIONED BY (
    data_type STRING,
    ymd STRING) 
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' 
STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 'org.apache.hadoop.mapred.TextOutputFormat' 
LOCATION 'oss://data-dlc/minimax-dialogue/data/chatml/';
```

2. parquet格式
   ```sql
   CREATE EXTERNAL TABLE dwd_crawler_html_out_di
   (
      url string,
       html string,
       source_tag string,
       lang string
   )
   partitioned BY (ymd string)
   stored AS parquet  
   location 'oss://data-dlc/minimax-dialogue/data/chatml/data_type=html_data/';
   ```
3. 自定义解析器
```sql
CREATE EXTERNAL TABLE ods_crawler_sites_warc_data_extract_result_external_hi (
    warc_host string,
    meta_info string,
    final_url string,
    status_code string,
    data_dt string,
    biz_code string,
    target_url string,
    warc_date string,
    content_type string,
    content_length string,
    warc_file_name string,
    content string,
    import_msg string
)
PARTITIONED BY (
    host string,
    status string,
    ymd string,
    hour string
)
stored BY 'com.crawler.udf.sites_warc.WarcStorageHandler' 
with serdeproperties (
    'max_file_size' = '157286400'
) 
LOCATION 'oss://crawler-data-storage/Crawlerlee/'
USING 'crawler_udf-1.0-SNAPSHOT.jar'
```
可参考这篇文章

### 2. 使用
1. 插入数据
```sql
SET odps.sql.unstructured.oss.commit.mode = true; -- 防止生成.odps文件夹
INSERT OVERWRITE TABLE dwd_crawler_html_out_di PARTITION(ymd='${P_DATE}')
SELECT 
    a.source_url,
    html,
    biz_code,
    parse_lang
FROM 
    dwd_crawler_content_filtered_detail_byscore_di a
WHERE 
    a.ymd = '${P_DATE}'
    AND a.score_level != 'low'
    AND COALESCE(html, '') != ''
```
2. 读取数据
```
-- 扫描所有路径
msck TABLE ods_crawler_sites_warc_data_extract_result_external_hi ADD partitions;
-- 扫描指定路径
ALTER TABLE ods_crawler_sites_warc_data_extract_result_external_hi ADD IF NOT EXISTS 
PARTITION (host='demo.com',status='error',ymd='20250624', hour='09') 
PARTITION (host='demo.com',status='success',ymd='20250624', hour='09')
;
-- 指定location
ALTER TABLE log_table_external ADD PARTITION (year = '2016', month = '06', day = '01')
location 'oss://oss-cn-hangzhou-internal.aliyuncs.com/bucket名称/oss-odps-test/log_data_customized/2016/06/01/';
-- 指定路径映射
MSCK REPAIR TABLE  ods_crawler_sites_warc_data_extract_result_external_hi ADD PARTITIONS
WITH PROPERTIES ('odps.msck.partition.column.mapping'='host:host,status:status,ymd:ymd');
```。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/MaxCompute-wai-biao-shi-yong.html</guid><pubDate>Wed, 25 Jun 2025 07:38:39 +0000</pubDate></item><item><title>表命名规范</title><link>https://enderTree.github.io/blog/post/biao-ming-ming-gui-fan.html</link><description>
### 1. 数仓分层
按 ods -&gt; dwd -&gt; dwm -&gt; dws -&gt;ads 来做
- dwd：事实级别的明细，或者did uid级的数据。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/biao-ming-ming-gui-fan.html</guid><pubDate>Wed, 25 Jun 2025 07:30:17 +0000</pubDate></item><item><title>全局排序优化</title><link>https://enderTree.github.io/blog/post/quan-ju-pai-xu-you-hua.html</link><description>```sql

CREATE TABLE talkie_dialogue.tmp_nana_data_1028_555 AS 
WITH hash_bucket AS (
    SELECT  
        *,
        ROW_NUMBER() OVER (PARTITION BY bucket_no ORDER BY user_msg_log_ucnt_30d ASC ) AS bucket_rel_index,
        COUNT(1) OVER (PARTITION BY bucket_no ) AS bucket_size
    FROM    
    (
        SELECT  
            *,
            ABS(HASH(cid)) % 100000 AS bucket_no 
        FROM    
            talkie_dialogue.tmp_nana_data_1028_4            
    ) 
),
bucket_base AS 
(
    SELECT
        bucket_no
        SUM(bucket_size) OVER (ORDER BY bucket_no ASC) - bucket_size AS bucket_base
    FROM
    (
        SELECT 
            bucket_no,
            bucket_size
        FROM
            hash_bucket
    )
)
SELECT
    /*+ MAPJOIN(t2) */ t1.*,
    t2.bucket_base + bucket_rel_index AS id_index
FROM
    hash_bucket t1 
JOIN 
    bucket_base t2 
    ON t1.bucket_no = t2.bucket_no
    AND t1.mid_rank = t2.mid_rank
    AND t1.country_rank = t2.country_rank
    AND t1.setting_type = t2.setting_type
;
```

核心代码如上，主要思路先做分桶进行分布式局部排序、再基于桶大小得到全局索引

1. 第一步先对id做hash分桶，然后计算每个桶的大小，桶数量可以根据需要计算的id数量来评估，这里分100000个桶，然后计算出每个id在桶内的相对位置bucket_rel_index，同时计算出桶大小bucket_size；
2. 第二步根据桶大小计算每个桶的基址；
3. 第三步将桶基址+id在桶内的相对地址得到全局唯一的绝对地址id_index；

。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/quan-ju-pai-xu-you-hua.html</guid><pubDate>Wed, 25 Jun 2025 03:39:22 +0000</pubDate></item><item><title>解决Join数据倾斜</title><link>https://enderTree.github.io/blog/post/jie-jue-Join-shu-ju-qing-xie.html</link><description>优化前：
```sql
SELECT 
        /*+ mapjoin(dd),skewjoin(a(url))*/
        a.* except(url_tag, join_key),
        CASE 
            WHEN cc.url IS NOT NULL THEN 'black_url'
            WHEN dd.url IS NOT NULL THEN 'black_url'
            ELSE ''
        END AS url_tag
    FROM
    (
        select 
            *,
            1 AS join_key
        from 
            dwd_crawler_urls_step1_di a 
        where 
            ymd = '${P_DATE}'
            AND url_tag != 'black_url'
    ) as
    LEFT JOIN 
        black_urls2 cc  
        ON replace(a.host, 'www.', '') = replace(cc.url, 'www.', '')
	LEFT JOIN 
        black_urls1 dd  
        ON INSTR(a.url, dd.url) &gt; 0
```
优化后
```sql
SELECT 
        /*+ mapjoin(dd),skewjoin(a(url))*/
        a.* except(url_tag, join_key),
        CASE 
            WHEN cc.url IS NOT NULL THEN 'black_url'
            WHEN dd.url IS NOT NULL THEN 'black_url'
            ELSE ''
        END AS url_tag
    FROM
    (
        select 
            *,
            1 AS join_key
        from 
            dwd_crawler_urls_step1_di a 
        where 
            ymd = '${P_DATE}'
            AND url_tag != 'black_url'
    ) a
    LEFT JOIN 
        black_urls1 dd  
        ON INSTR(a.url, dd.url) &gt; 0
    LEFT JOIN 
        black_urls2 cc  
        ON replace(a.host, 'www.', '') = replace(cc.url, 'www.', '')
```
优化前执行时间

&lt;img width='991' alt='Image' src='https://github.com/user-attachments/assets/3d62cdfd-b1e0-4579-adc9-a8bc19ac64ff' /&gt;

&lt;img width='872' alt='Image' src='https://github.com/user-attachments/assets/46033d2f-9fbe-4400-b6c6-489f97ee7f00' /&gt;

优化后执行时间

&lt;img width='978' alt='Image' src='https://github.com/user-attachments/assets/74de5fc3-08ff-4ed6-8c94-9b0d31c18799' /&gt;

&lt;img width='944' alt='Image' src='https://github.com/user-attachments/assets/b2f397da-420d-484b-9655-3dda6ee57046' /&gt;
。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/jie-jue-Join-shu-ju-qing-xie.html</guid><pubDate>Wed, 25 Jun 2025 03:39:03 +0000</pubDate></item><item><title>窗口函数的一些使用</title><link>https://enderTree.github.io/blog/post/chuang-kou-han-shu-de-yi-xie-shi-yong.html</link><description>#### 1. 取最近一个不为空的值填充
```sql
WITH mian_data AS (
    SELECT
        c0,
        c1,
        IF(c1 = '处置', NULL, c2) AS c2
    FROM
        VALUES
         (1,'处置','1')
        ,(2,'处置','2')
        ,(3,'处置','3')
        ,(4,'处置','4')
        ,(5,'提交','A')
        ,(6,'处置','5')
        ,(7,'提交','b')
    AS a(c0,c1,c2)
)
SELECT 
    *,
    FIRST_VALUE(c2, True) over(order by c0 ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)
FROM 
    mian_data 
```。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/chuang-kou-han-shu-de-yi-xie-shi-yong.html</guid><pubDate>Wed, 25 Jun 2025 03:38:36 +0000</pubDate></item><item><title>个人简历</title><link>https://enderTree.github.io/blog/resume.html</link><description># 个人信息

- 冷熊/男/1990
- 本科/北极大学计算机系
- 工作年限：3年
- 微博：[@Easy](http://weibo.com/easy) （如果没有技术相关内容，也可以不放）
- 技术博客：http://old.ftqq.com ( 使用GitHub Host的Big较高  )
- Github：http://github.com/easychen ( 有原创repo的Github帐号会极大的提升你的个人品牌  )
- 期望职位：PHP高级程序员，应用架构师
- 期望薪资：税前月薪15k~20k，特别喜欢的公司可例外
- 期望城市：北京

# 工作经历

（工作经历按逆序排列，最新的在最前边，按公司做一级分组，公司内按二级分组）

## ABC公司 （ 2012年9月 ~ 2014年9月 ）

### DEF项目

我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。</description><guid isPermaLink="true">https://enderTree.github.io/blog/resume.html</guid><pubDate>Wed, 25 Jun 2025 11:40:41 +0000</pubDate></item></channel></rss>