<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>ender'Blog</title><link>https://enderTree.github.io/blog</link><description>闭关ING</description><copyright>ender'Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://enderTree.github.io/blog</link></image><lastBuildDate>Thu, 26 Jun 2025 09:12:11 +0000</lastBuildDate><managingEditor>ender'Blog</managingEditor><ttl>60</ttl><webMaster>ender'Blog</webMaster><item><title>使用MaxFrame将oss文件入库</title><link>https://enderTree.github.io/blog/post/shi-yong-MaxFrame-jiang-oss-wen-jian-ru-ku.html</link><description>1. oss配置
```python
OSS_ACCESS_ID = ''  # OSS 的 Access ID
OSS_SECRET_ACCESS_KEY = ''  # OSS 的 Secret ID

# 填写 Bucket 信息
OSS_INTERNET_ENDPOINT = (
    'oss-cn-shanghai.aliyuncs.com'  # OSS 的公网访问 endpoint，您也可以使用 sts token
)
OSS_INTERNAL_ENDPOINT = 'oss-cn-shanghai-internal.aliyuncs.com'  # OSS 的公网访问 endpoint，您也可以使用 sts token
OSS_BUCKET_NAME = 'crawler-data-storage'  # OSS Bucket 的名字
OSS_BUCKET_ENDPOINT = f'{OSS_BUCKET_NAME}.{OSS_INTERNET_ENDPOINT}:80'
OSS_BUCKET_ENDPOINT

import oss2
from oss2 import defaults
defaults.connection_pool_size = 50

auth = oss2.Auth(OSS_ACCESS_ID, OSS_SECRET_ACCESS_KEY)
bucket = oss2.Bucket(auth, OSS_INTERNET_ENDPOINT, OSS_BUCKET_NAME)
```
2. 定义oss文件扫描方法
```python
import oss2
from concurrent.futures import ThreadPoolExecutor, as_completed


def get_oss_files(bucket, prefix, ext='.snappy.parquet', max_size=1 * 1024 * 1024 * 1024 * 1024):
    '''
    使用多线程遍历 OSS 路径下的所有目录。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/shi-yong-MaxFrame-jiang-oss-wen-jian-ru-ku.html</guid><pubDate>Thu, 26 Jun 2025 09:11:43 +0000</pubDate></item><item><title>阿里云个人使用的一些姿势</title><link>https://enderTree.github.io/blog/post/a-li-yun-ge-ren-shi-yong-de-yi-xie-zi-shi.html</link><description># 阿里云个人使用的一些姿势

## 一、登录

[登录 dataworks](https://dataworks.console.aliyun.com/workspace/list)，选择对应工作空间进入，显示无空间找管理员开通

&lt;img width='1693' alt='Image' src='https://github.com/user-attachments/assets/b2e470eb-06e2-4c2c-8d12-da6232adc7b2' /&gt;

&lt;img width='1238' alt='Image' src='https://github.com/user-attachments/assets/2408c27a-fcf3-4631-a68a-2f3005c49936' /&gt;

## 二、临时 Sql 运行

临时查询下新建一个目录，在下面新建 odps sql 节点即可

## 三、周期任务

### sql 任务

业务流程 MaxCompute 数据开发目录下新建 ODPS Sql 节点

### PySpark 简单模式

[官方文档](https://help.aliyun.com/zh/maxcompute/user-guide/develop-a-spark-on-maxcompute-application-by-using-pyspark?spm=a2c4g.11186623.0.i2)

1. 资源中新建 python 脚本
   
&lt;img width='443' alt='Image' src='https://github.com/user-attachments/assets/9e325886-5153-4c75-b19b-23fa6476f1b6' /&gt;

 ```python
from pyspark.sql import SparkSession
if __name__ == '__main__':
spark = SparkSession.builder.appName('spark sql').getOrCreate()
spark.sql('SELECT 1').show()
```
2. 数据开发文件夹新建odps spark节点

&lt;img width='735' alt='Image' src='https://github.com/user-attachments/assets/e4a30db5-a9b3-4110-bd83-85c764450f39' /&gt;

发布上线皆可，sparkui会在日志中打印

### PySpark兼容模式

阿里云的PySpark是基于Python 3.7 + Spark2的，而且看上去已经疏于维护了。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/a-li-yun-ge-ren-shi-yong-de-yi-xie-zi-shi.html</guid><pubDate>Wed, 25 Jun 2025 11:06:12 +0000</pubDate></item><item><title>MaxCompute外表使用</title><link>https://enderTree.github.io/blog/post/MaxCompute-wai-biao-shi-yong.html</link><description>### 1. 建表
1. json格式

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS dialogue.dwd_ext_xingye_log_chatml_data(
    `data` STRING
) 
PARTITIONED BY (
    data_type STRING,
    ymd STRING) 
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' 
STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 'org.apache.hadoop.mapred.TextOutputFormat' 
LOCATION 'oss://data-dlc/minimax-dialogue/data/chatml/';
```

2. parquet格式
   ```sql
   CREATE EXTERNAL TABLE dwd_crawler_html_out_di
   (
      url string,
       html string,
       source_tag string,
       lang string
   )
   partitioned BY (ymd string)
   stored AS parquet  
   location 'oss://data-dlc/minimax-dialogue/data/chatml/data_type=html_data/';
   ```
3. 自定义解析器
```sql
CREATE EXTERNAL TABLE ods_crawler_sites_warc_data_extract_result_external_hi (
    warc_host string,
    meta_info string,
    final_url string,
    status_code string,
    data_dt string,
    biz_code string,
    target_url string,
    warc_date string,
    content_type string,
    content_length string,
    warc_file_name string,
    content string,
    import_msg string
)
PARTITIONED BY (
    host string,
    status string,
    ymd string,
    hour string
)
stored BY 'com.crawler.udf.sites_warc.WarcStorageHandler' 
with serdeproperties (
    'max_file_size' = '157286400'
) 
LOCATION 'oss://crawler-data-storage/Crawlerlee/'
USING 'crawler_udf-1.0-SNAPSHOT.jar'
```
可参考这篇文章

### 2. 使用
1. 插入数据
```sql
SET odps.sql.unstructured.oss.commit.mode = true; -- 防止生成.odps文件夹
INSERT OVERWRITE TABLE dwd_crawler_html_out_di PARTITION(ymd='${P_DATE}')
SELECT 
    a.source_url,
    html,
    biz_code,
    parse_lang
FROM 
    dwd_crawler_content_filtered_detail_byscore_di a
WHERE 
    a.ymd = '${P_DATE}'
    AND a.score_level != 'low'
    AND COALESCE(html, '') != ''
```
2. 读取数据
```
-- 扫描所有路径
msck TABLE ods_crawler_sites_warc_data_extract_result_external_hi ADD partitions;
-- 扫描指定路径
ALTER TABLE ods_crawler_sites_warc_data_extract_result_external_hi ADD IF NOT EXISTS 
PARTITION (host='demo.com',status='error',ymd='20250624', hour='09') 
PARTITION (host='demo.com',status='success',ymd='20250624', hour='09')
;
-- 指定location
ALTER TABLE log_table_external ADD PARTITION (year = '2016', month = '06', day = '01')
location 'oss://oss-cn-hangzhou-internal.aliyuncs.com/bucket名称/oss-odps-test/log_data_customized/2016/06/01/';
-- 指定路径映射
MSCK REPAIR TABLE  ods_crawler_sites_warc_data_extract_result_external_hi ADD PARTITIONS
WITH PROPERTIES ('odps.msck.partition.column.mapping'='host:host,status:status,ymd:ymd');
```。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/MaxCompute-wai-biao-shi-yong.html</guid><pubDate>Wed, 25 Jun 2025 07:38:39 +0000</pubDate></item><item><title>表命名规范</title><link>https://enderTree.github.io/blog/post/biao-ming-ming-gui-fan.html</link><description>
### 1. 数仓分层
按 ods -&gt; dwd -&gt; dwm -&gt; dws -&gt;ads 来做
- dwd：事实级别的明细，或者did uid级的数据。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/biao-ming-ming-gui-fan.html</guid><pubDate>Wed, 25 Jun 2025 07:30:17 +0000</pubDate></item><item><title>全局排序优化</title><link>https://enderTree.github.io/blog/post/quan-ju-pai-xu-you-hua.html</link><description>```sql

CREATE TABLE talkie_dialogue.tmp_nana_data_1028_555 AS 
WITH hash_bucket AS (
    SELECT  
        *,
        ROW_NUMBER() OVER (PARTITION BY bucket_no ORDER BY user_msg_log_ucnt_30d ASC ) AS bucket_rel_index,
        COUNT(1) OVER (PARTITION BY bucket_no ) AS bucket_size
    FROM    
    (
        SELECT  
            *,
            ABS(HASH(cid)) % 100000 AS bucket_no 
        FROM    
            talkie_dialogue.tmp_nana_data_1028_4            
    ) 
),
bucket_base AS 
(
    SELECT
        bucket_no
        SUM(bucket_size) OVER (ORDER BY bucket_no ASC) - bucket_size AS bucket_base
    FROM
    (
        SELECT 
            bucket_no,
            bucket_size
        FROM
            hash_bucket
    )
)
SELECT
    /*+ MAPJOIN(t2) */ t1.*,
    t2.bucket_base + bucket_rel_index AS id_index
FROM
    hash_bucket t1 
JOIN 
    bucket_base t2 
    ON t1.bucket_no = t2.bucket_no
    AND t1.mid_rank = t2.mid_rank
    AND t1.country_rank = t2.country_rank
    AND t1.setting_type = t2.setting_type
;
```

核心代码如上，主要思路先做分桶进行分布式局部排序、再基于桶大小得到全局索引

1. 第一步先对id做hash分桶，然后计算每个桶的大小，桶数量可以根据需要计算的id数量来评估，这里分100000个桶，然后计算出每个id在桶内的相对位置bucket_rel_index，同时计算出桶大小bucket_size；
2. 第二步根据桶大小计算每个桶的基址；
3. 第三步将桶基址+id在桶内的相对地址得到全局唯一的绝对地址id_index；

。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/quan-ju-pai-xu-you-hua.html</guid><pubDate>Wed, 25 Jun 2025 03:39:22 +0000</pubDate></item><item><title>解决Join数据倾斜</title><link>https://enderTree.github.io/blog/post/jie-jue-Join-shu-ju-qing-xie.html</link><description>优化前：
```sql
SELECT 
        /*+ mapjoin(dd),skewjoin(a(url))*/
        a.* except(url_tag, join_key),
        CASE 
            WHEN cc.url IS NOT NULL THEN 'black_url'
            WHEN dd.url IS NOT NULL THEN 'black_url'
            ELSE ''
        END AS url_tag
    FROM
    (
        select 
            *,
            1 AS join_key
        from 
            dwd_crawler_urls_step1_di a 
        where 
            ymd = '${P_DATE}'
            AND url_tag != 'black_url'
    ) as
    LEFT JOIN 
        black_urls2 cc  
        ON replace(a.host, 'www.', '') = replace(cc.url, 'www.', '')
	LEFT JOIN 
        black_urls1 dd  
        ON INSTR(a.url, dd.url) &gt; 0
```
优化后
```sql
SELECT 
        /*+ mapjoin(dd),skewjoin(a(url))*/
        a.* except(url_tag, join_key),
        CASE 
            WHEN cc.url IS NOT NULL THEN 'black_url'
            WHEN dd.url IS NOT NULL THEN 'black_url'
            ELSE ''
        END AS url_tag
    FROM
    (
        select 
            *,
            1 AS join_key
        from 
            dwd_crawler_urls_step1_di a 
        where 
            ymd = '${P_DATE}'
            AND url_tag != 'black_url'
    ) a
    LEFT JOIN 
        black_urls1 dd  
        ON INSTR(a.url, dd.url) &gt; 0
    LEFT JOIN 
        black_urls2 cc  
        ON replace(a.host, 'www.', '') = replace(cc.url, 'www.', '')
```
优化前执行时间

&lt;img width='991' alt='Image' src='https://github.com/user-attachments/assets/3d62cdfd-b1e0-4579-adc9-a8bc19ac64ff' /&gt;

&lt;img width='872' alt='Image' src='https://github.com/user-attachments/assets/46033d2f-9fbe-4400-b6c6-489f97ee7f00' /&gt;

优化后执行时间

&lt;img width='978' alt='Image' src='https://github.com/user-attachments/assets/74de5fc3-08ff-4ed6-8c94-9b0d31c18799' /&gt;

&lt;img width='944' alt='Image' src='https://github.com/user-attachments/assets/b2f397da-420d-484b-9655-3dda6ee57046' /&gt;
。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/jie-jue-Join-shu-ju-qing-xie.html</guid><pubDate>Wed, 25 Jun 2025 03:39:03 +0000</pubDate></item><item><title>窗口函数的一些使用</title><link>https://enderTree.github.io/blog/post/chuang-kou-han-shu-de-yi-xie-shi-yong.html</link><description>#### 1. 取最近一个不为空的值填充
```sql
WITH mian_data AS (
    SELECT
        c0,
        c1,
        IF(c1 = '处置', NULL, c2) AS c2
    FROM
        VALUES
         (1,'处置','1')
        ,(2,'处置','2')
        ,(3,'处置','3')
        ,(4,'处置','4')
        ,(5,'提交','A')
        ,(6,'处置','5')
        ,(7,'提交','b')
    AS a(c0,c1,c2)
)
SELECT 
    *,
    FIRST_VALUE(c2, True) over(order by c0 ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)
FROM 
    mian_data 
```。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/chuang-kou-han-shu-de-yi-xie-shi-yong.html</guid><pubDate>Wed, 25 Jun 2025 03:38:36 +0000</pubDate></item><item><title>test</title><link>https://enderTree.github.io/blog/post/test.html</link><description>```sql

SELECT
    *
FROM
(

    SELECT
        tag,
        doc_cnt,
        (doc_cnt/SUM(doc_cnt) OVER())*100
    FROM
    (
        SELECT 
            tag,
            SUM(doc_cnt) AS doc_cnt
        FROM
            dwd_crawler_host_classification_di a
        LEFT ANTI JOIN 
            tmp_gumu_data_0624_2 b
            ON a.host = b.host
        WHERE
            ymd = '2025-06-23'
            AND tag_type = 'topic'
        GROUP BY 
            tag
    )
)
WHERE 
    tag = 'Advertising_&amp;_Marketing'
;
```。</description><guid isPermaLink="true">https://enderTree.github.io/blog/post/test.html</guid><pubDate>Wed, 25 Jun 2025 02:50:59 +0000</pubDate></item><item><title>个人简历</title><link>https://enderTree.github.io/blog/resume.html</link><description># 个人信息

- 冷熊/男/1990
- 本科/北极大学计算机系
- 工作年限：3年
- 微博：[@Easy](http://weibo.com/easy) （如果没有技术相关内容，也可以不放）
- 技术博客：http://old.ftqq.com ( 使用GitHub Host的Big较高  )
- Github：http://github.com/easychen ( 有原创repo的Github帐号会极大的提升你的个人品牌  )
- 期望职位：PHP高级程序员，应用架构师
- 期望薪资：税前月薪15k~20k，特别喜欢的公司可例外
- 期望城市：北京

# 工作经历

（工作经历按逆序排列，最新的在最前边，按公司做一级分组，公司内按二级分组）

## ABC公司 （ 2012年9月 ~ 2014年9月 ）

### DEF项目

我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。</description><guid isPermaLink="true">https://enderTree.github.io/blog/resume.html</guid><pubDate>Wed, 25 Jun 2025 11:40:41 +0000</pubDate></item></channel></rss>